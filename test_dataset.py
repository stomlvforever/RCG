#!/usr/bin/env python3
"""
ç®€åŒ–çš„å¤–éƒ¨æµ‹è¯•è„šæœ¬
æµ‹è¯• performat_SramDataset å‡½æ•°
"""

import os
import sys
import torch

# å¯¼å…¥æ•°æ®é›†å‡½æ•°
try:
    from dataset import performat_SramDataset
    print("âœ… æˆåŠŸå¯¼å…¥ performat_SramDataset å‡½æ•°")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ dataset.py æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹")
    sys.exit(1)

def prepare_files():
    """æ£€æŸ¥æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    print("ğŸ”§ æ£€æŸ¥æµ‹è¯•æ–‡ä»¶...")
    
    files = [
        "/data/tianjn/baseline/basline/basline/sram/raw/integrated_position_prediction_graph.pt",
        "/data/tianjn/baseline/basline/basline/sram/raw/integrated_power_density_prediction_graph.pt"
    ]
    
    for file_path in files:
        if os.path.exists(file_path):
            print(f"âœ… æ–‡ä»¶å­˜åœ¨: {os.path.basename(file_path)}")
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
    
    return True

def get_feature_names(sample):
    """è·å–ç‰¹å¾åç§°"""
    # å°è¯•ä»æ ·æœ¬ä¸­è·å–ç‰¹å¾åç§°
    
    # æ–¹æ³•1: æ£€æŸ¥æ˜¯å¦æœ‰å­˜å‚¨çš„ç‰¹å¾åç§°
    if hasattr(sample, 'feature_names'):
        return sample.feature_names
    
    # æ–¹æ³•2: æ£€æŸ¥æ˜¯å¦æœ‰å…ƒæ•°æ®
    if hasattr(sample, 'metadata') and isinstance(sample.metadata, dict):
        if 'feature_names' in sample.metadata:
            return sample.metadata['feature_names']
    
    # æ–¹æ³•3: æ£€æŸ¥åŸå§‹å›¾æ•°æ®ä¸­æ˜¯å¦æœ‰ç‰¹å¾åç§°ä¿¡æ¯
    if hasattr(sample, '_node_attr_names'):
        return sample._node_attr_names
    
    # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›None
    return None

def analyze_dataset_attributes(dataset, inspect_attr=None, detailed_tensor='x'):
    """è¯¦ç»†åˆ†ææ•°æ®é›†å¯¹è±¡çš„æ‰€æœ‰å±æ€§"""
    print(f"\n{'='*60}")
    print("ğŸ” æ•°æ®é›†å¯¹è±¡å±æ€§è¯¦ç»†åˆ†æ")
    print(f"{'='*60}")
    
    # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œåˆ†æ
    if len(dataset) > 0:
        sample = dataset[0]
        print(f"ğŸ“Š æ ·æœ¬å¯¹è±¡ç±»å‹: {type(sample)}")
        print(f"ğŸ“Š æ ·æœ¬å¯¹è±¡çš„æ‰€æœ‰å±æ€§:")
        
        # æ”¶é›†æ‰€æœ‰å±æ€§
        all_attrs = []
        for attr_name in dir(sample):
            if not attr_name.startswith('_'):
                try:
                    attr_value = getattr(sample, attr_name)
                    if not callable(attr_value):
                        all_attrs.append((attr_name, attr_value))
                except Exception as e:
                    print(f"âŒ è·å–å±æ€§ '{attr_name}' æ—¶å‡ºé”™: {e}")
                    continue
        
        # æ‰“å°åŸºæœ¬å±æ€§åˆ—è¡¨
        print(f"\nğŸ“‹ å‘ç° {len(all_attrs)} ä¸ªå±æ€§:")
        for i, (attr_name, attr_value) in enumerate(all_attrs, 1):
            if hasattr(attr_value, 'shape'):
                print(f"   {i:2d}. {attr_name}: {attr_value.shape} ({attr_value.dtype})")
            elif hasattr(attr_value, '__len__') and not isinstance(attr_value, str):
                print(f"   {i:2d}. {attr_name}: é•¿åº¦={len(attr_value)} ({type(attr_value).__name__})")
            else:
                print(f"   {i:2d}. {attr_name}: {type(attr_value).__name__}")
        
        # è¯¦ç»†åˆ†ææ¯ä¸ªå¼ é‡å±æ€§
        print(f"\nğŸ“Š å¼ é‡å±æ€§è¯¦ç»†ä¿¡æ¯:")
        tensor_attrs = [(name, val) for name, val in all_attrs if hasattr(val, 'shape')]
        
        for attr_name, attr_value in tensor_attrs:
            print(f"\n   ğŸ”¹ {attr_name}:")
            print(f"      - å½¢çŠ¶: {attr_value.shape}")
            print(f"      - æ•°æ®ç±»å‹: {attr_value.dtype}")
            if attr_value.numel() > 0:  # éç©ºå¼ é‡
                print(f"      - å€¼èŒƒå›´: [{attr_value.min().item():.4f}, {attr_value.max().item():.4f}]")
                print(f"      - å…ƒç´ æ€»æ•°: {attr_value.numel()}")
                if len(attr_value.shape) > 1:
                    print(f"      - æ¯ä¸ªç»´åº¦: {list(attr_value.shape)}")
            
        # è¯¦ç»†åˆ†ææ¯ä¸ªå¼ é‡å±æ€§
        print(f"\nğŸ“Š å¼ é‡å±æ€§è¯¦ç»†ä¿¡æ¯:")
        print(f"   ğŸ’¡ åªå¯¹ '{detailed_tensor}' æ˜¾ç¤ºè¯¦ç»†å­å±æ€§ä¿¡æ¯ï¼Œå…¶ä»–å¼ é‡åªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯")
        
        tensor_attrs = [(name, val) for name, val in all_attrs if hasattr(val, 'shape')]
        
        for attr_name, attr_value in tensor_attrs:
            print(f"\n   ğŸ”¹ {attr_name}:")
            print(f"      - å½¢çŠ¶: {attr_value.shape}")
            print(f"      - æ•°æ®ç±»å‹: {attr_value.dtype}")
            if attr_value.numel() > 0:  # éç©ºå¼ é‡
                print(f"      - å€¼èŒƒå›´: [{attr_value.min().item():.4f}, {attr_value.max().item():.4f}]")
                print(f"      - å…ƒç´ æ€»æ•°: {attr_value.numel()}")
                if len(attr_value.shape) > 1:
                    print(f"      - æ¯ä¸ªç»´åº¦: {list(attr_value.shape)}")
            
            # åªå¯¹æŒ‡å®šçš„å¼ é‡æ˜¾ç¤ºè¯¦ç»†å­å±æ€§ä¿¡æ¯
            if attr_name == detailed_tensor:
                print(f"      ğŸ” è¯¦ç»†åˆ†æ {attr_name} çš„å­å±æ€§:")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å­å±æ€§
                sub_attrs = []
                for sub_attr_name in dir(attr_value):
                    if not sub_attr_name.startswith('_'):
                        try:
                            sub_attr_value = getattr(attr_value, sub_attr_name)
                            if not callable(sub_attr_value) and not hasattr(sub_attr_value, 'shape'):
                                sub_attrs.append((sub_attr_name, sub_attr_value))
                        except Exception as e:
                            print(f"         âŒ è·å–å¼ é‡ '{attr_name}' çš„å­å±æ€§ '{sub_attr_name}' æ—¶å‡ºé”™: {e}")
                            continue
                
                if sub_attrs:
                    print(f"         - å­å±æ€§: {len(sub_attrs)} ä¸ª")
                    for sub_name, sub_val in sub_attrs[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                        print(f"           * {sub_name}: {type(sub_val).__name__}")
                        if isinstance(sub_val, (list, tuple)) and len(sub_val) <= 20:
                            print(f"             å†…å®¹: {sub_val}")
                        elif hasattr(sub_val, '__len__') and not isinstance(sub_val, str):
                            print(f"             é•¿åº¦: {len(sub_val)}")
                        elif isinstance(sub_val, (str, int, float, bool)):
                            print(f"             å€¼: {sub_val}")
                else:
                    print(f"         - å­å±æ€§: æ— ")
                    
                # å¯¹æŒ‡å®šå¼ é‡è¿›è¡Œç‰¹å¾åç§°æŸ¥æ‰¾å’Œæ•°æ®åˆ†æ
                print(f"\n      ğŸ” æŸ¥æ‰¾å¼ é‡ '{detailed_tensor}' çš„ç‰¹å¾åç§°:")
                print(f"         ğŸ“Š {detailed_tensor}å¼ é‡å½¢çŠ¶: {attr_value.shape}")
                
                # æŸ¥æ‰¾å¯èƒ½åŒ…å«ç‰¹å¾åç§°çš„å±æ€§
                print(f"         ğŸ” æ£€æŸ¥{detailed_tensor}å¼ é‡çš„æ‰€æœ‰å­å±æ€§:")
                detailed_attrs = []
                for sub_attr_name in dir(attr_value):
                    if not sub_attr_name.startswith('__'):  # æ’é™¤é­”æœ¯æ–¹æ³•
                        try:
                            sub_attr_value = getattr(attr_value, sub_attr_name)
                            if not callable(sub_attr_value):
                                detailed_attrs.append((sub_attr_name, sub_attr_value))
                                print(f"            - {sub_attr_name}: {type(sub_attr_value)}")
                                
                                # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯ç‰¹å¾åç§°
                                if isinstance(sub_attr_value, (list, tuple)) and len(sub_attr_value) == attr_value.shape[-1]:
                                    print(f"              ğŸ’¡ å¯èƒ½æ˜¯ç‰¹å¾åç§°! é•¿åº¦åŒ¹é…æœ€åç»´åº¦æ•° ({len(sub_attr_value)})")
                                    print(f"              å†…å®¹: {sub_attr_value}")
                                elif hasattr(sub_attr_value, '__len__') and not isinstance(sub_attr_value, str):
                                    if len(sub_attr_value) <= 20:  # ä¸è¦æ‰“å°å¤ªé•¿çš„
                                        print(f"              é•¿åº¦: {len(sub_attr_value)}, å†…å®¹: {sub_attr_value}")
                                    else:
                                        print(f"              é•¿åº¦: {len(sub_attr_value)} (å¤ªé•¿ä¸æ˜¾ç¤º)")
                                elif isinstance(sub_attr_value, (str, int, float, bool)):
                                    print(f"              å€¼: {sub_attr_value}")
                        except Exception as e:
                            if any(keyword in str(e).lower() for keyword in ['imag', 'complex', '.h', '.mh', '.mt']):
                                continue  # è·³è¿‡å·²çŸ¥çš„å¼ é‡å±æ€§é”™è¯¯
                            print(f"            âŒ {sub_attr_name}: {e}")
                
                print(f"\n         ğŸ“‹ {detailed_tensor}å¼ é‡å…±æœ‰ {len(detailed_attrs)} ä¸ªå¯è®¿é—®çš„å­å±æ€§")
                
                # æ£€æŸ¥sampleçº§åˆ«æ˜¯å¦æœ‰å¯¹åº”çš„ç‰¹å¾åç§°ä¿¡æ¯
                print(f"\n         ğŸ” æ£€æŸ¥sampleçº§åˆ«çš„{detailed_tensor}ç‰¹å¾åç§°ä¿¡æ¯:")
                sample_attrs = []
                search_keywords = ['name', 'feature', 'column', 'header']
                if detailed_tensor == 'edge_attr':
                    search_keywords.extend(['edge'])
                elif detailed_tensor == 'x':
                    search_keywords.extend(['node'])
                elif detailed_tensor == 'y':
                    search_keywords.extend(['label', 'target'])
                
                for sample_attr_name in dir(sample):
                    if any(keyword in sample_attr_name.lower() for keyword in search_keywords):
                        if not sample_attr_name.startswith('_') and not callable(getattr(sample, sample_attr_name)):
                            try:
                                sample_attr_value = getattr(sample, sample_attr_name)
                                sample_attrs.append((sample_attr_name, sample_attr_value))
                                print(f"            - {sample_attr_name}: {type(sample_attr_value)}")
                                
                                if isinstance(sample_attr_value, (list, tuple)):
                                    print(f"              é•¿åº¦: {len(sample_attr_value)}")
                                    if len(sample_attr_value) <= 20:
                                        print(f"              å†…å®¹: {sample_attr_value}")
                                    elif len(sample_attr_value) == attr_value.shape[-1]:
                                        print(f"              ğŸ’¡ é•¿åº¦åŒ¹é…{detailed_tensor}çš„æœ€åç»´åº¦æ•°! å†…å®¹: {sample_attr_value}")
                                elif hasattr(sample_attr_value, '__len__') and not isinstance(sample_attr_value, str):
                                    print(f"              é•¿åº¦: {len(sample_attr_value)}")
                                else:
                                    print(f"              å€¼: {sample_attr_value}")
                            except Exception as e:
                                print(f"            âŒ {sample_attr_name}: {e}")
                
                if not sample_attrs:
                    print(f"            âŒ æ²¡æœ‰æ‰¾åˆ°æ˜æ˜¾çš„{detailed_tensor}ç‰¹å¾åç§°å±æ€§")
                    
                # åˆ†ææ•°æ®åˆ†å¸ƒæ¥æ¨æµ‹ç‰¹å¾å«ä¹‰ï¼ˆåªå¯¹åˆç†å¤§å°çš„ç»´åº¦ï¼‰
                if len(attr_value.shape) >= 1 and (len(attr_value.shape) == 1 or attr_value.shape[-1] <= 50):
                    print(f"\n         ğŸ” åˆ†æ{detailed_tensor}å„ç»´åº¦çš„æ•°æ®åˆ†å¸ƒæ¥æ¨æµ‹ç‰¹å¾å«ä¹‰:")
                    
                    if len(attr_value.shape) == 1:
                        # ä¸€ç»´å¼ é‡
                        unique_vals = attr_value.unique()
                        print(f"            å•ç»´å¼ é‡:")
                        print(f"              - å€¼èŒƒå›´: [{attr_value.min().item():.4f}, {attr_value.max().item():.4f}]")
                        print(f"              - å”¯ä¸€å€¼æ•°é‡: {unique_vals.numel()}")
                        
                        if unique_vals.numel() <= 10:
                            print(f"              - æ‰€æœ‰å”¯ä¸€å€¼: {unique_vals.tolist()}")
                            if torch.all(attr_value == attr_value.long().float()) and unique_vals.numel() <= len(dataset.names):
                                print(f"              ğŸ’¡ å¯èƒ½æ˜¯å›¾ID! (æ•´æ•°å€¼ï¼Œå”¯ä¸€å€¼æ•°é‡={unique_vals.numel()}, æ•°æ®é›†æ•°é‡={len(dataset.names)})")
                        elif unique_vals.numel() <= 100:
                            print(f"              - å‰10ä¸ªå”¯ä¸€å€¼: {unique_vals[:10].tolist()}")
                            if torch.all(attr_value == attr_value.long().float()):
                                print(f"              ğŸ’¡ å¯èƒ½æ˜¯ç±»åˆ«ç‰¹å¾ (æ•´æ•°å€¼)")
                        
                        print(f"              - å‰5ä¸ªå€¼: {attr_value[:5].tolist()}")
                    else:
                        # å¤šç»´å¼ é‡
                        for dim in range(min(attr_value.shape[-1], 20)):
                            if len(attr_value.shape) == 2:
                                feature_col = attr_value[:, dim]
                            else:
                                continue
                                
                            unique_vals = feature_col.unique()
                            
                            print(f"            ç»´åº¦ {dim}:")
                            print(f"              - å€¼èŒƒå›´: [{feature_col.min().item():.4f}, {feature_col.max().item():.4f}]")
                            print(f"              - å”¯ä¸€å€¼æ•°é‡: {unique_vals.numel()}")
                            
                            # ç‰¹æ®Šæƒ…å†µåˆ†æ
                            if unique_vals.numel() <= 10:
                                print(f"              - æ‰€æœ‰å”¯ä¸€å€¼: {unique_vals.tolist()}")
                                if torch.all(feature_col == feature_col.long().float()) and unique_vals.numel() <= len(dataset.names):
                                    print(f"              ğŸ’¡ å¯èƒ½æ˜¯å›¾ID! (æ•´æ•°å€¼ï¼Œå”¯ä¸€å€¼æ•°é‡={unique_vals.numel()}, æ•°æ®é›†æ•°é‡={len(dataset.names)})")
                            elif unique_vals.numel() <= 100:
                                print(f"              - å‰10ä¸ªå”¯ä¸€å€¼: {unique_vals[:10].tolist()}")
                                if torch.all(feature_col == feature_col.long().float()):
                                    print(f"              ğŸ’¡ å¯èƒ½æ˜¯ç±»åˆ«ç‰¹å¾ (æ•´æ•°å€¼)")
                            
                            print(f"              - å‰5ä¸ªå€¼: {feature_col[:5].tolist()}")
                else:
                    print(f"         âš ï¸  å¼ é‡ç»´åº¦å¤ªå¤§ï¼Œè·³è¿‡æ•°æ®åˆ†å¸ƒåˆ†æ")
            else:
                print(f"      - å­å±æ€§: å·²è·³è¿‡è¯¦ç»†åˆ†æ (åªåˆ†æ '{detailed_tensor}')")
        
        # éå¼ é‡å±æ€§
        non_tensor_attrs = [(name, val) for name, val in all_attrs if not hasattr(val, 'shape')]
        if non_tensor_attrs:
            print(f"\nğŸ“‹ éå¼ é‡å±æ€§:")
            for attr_name, attr_value in non_tensor_attrs:
                print(f"   ğŸ”¸ {attr_name}: {type(attr_value).__name__}")
                if hasattr(attr_value, '__len__') and not isinstance(attr_value, str):
                    print(f"      - é•¿åº¦: {len(attr_value)}")
                    # æ˜¾ç¤ºå­—å…¸ã€åˆ—è¡¨ç­‰å®¹å™¨çš„å†…å®¹
                    if isinstance(attr_value, dict):
                        print(f"      - å†…å®¹: {attr_value}")
                    elif isinstance(attr_value, (list, tuple)) and len(attr_value) <= 10:
                        print(f"      - å†…å®¹: {attr_value}")
                    elif isinstance(attr_value, (list, tuple)):
                        print(f"      - å‰5ä¸ªå…ƒç´ : {attr_value[:5]}")
                if isinstance(attr_value, (int, float, str, bool)):
                    print(f"      - å€¼: {attr_value}")
                elif attr_value is None:
                    print(f"      - å€¼: None")
        
        # è‡ªå®šä¹‰å±æ€§æ£€æŸ¥æ¥å£
        if inspect_attr:
            print(f"\nğŸ” è‡ªå®šä¹‰å±æ€§æ£€æŸ¥: '{inspect_attr}'")
            if hasattr(sample, inspect_attr):
                custom_attr = getattr(sample, inspect_attr)
                print(f"   - ç±»å‹: {type(custom_attr)}")
                if hasattr(custom_attr, 'shape'):
                    print(f"   - å½¢çŠ¶: {custom_attr.shape}")
                    print(f"   - æ•°æ®ç±»å‹: {custom_attr.dtype}")
                    if custom_attr.numel() > 0:
                        print(f"   - å€¼èŒƒå›´: [{custom_attr.min().item():.4f}, {custom_attr.max().item():.4f}]")
                        print(f"   - å‰5ä¸ªå€¼: {custom_attr.flatten()[:5].tolist()}")
                elif hasattr(custom_attr, '__len__'):
                    print(f"   - é•¿åº¦: {len(custom_attr)}")
                    print(f"   - å†…å®¹é¢„è§ˆ: {str(custom_attr)[:100]}...")
                else:
                    print(f"   - å€¼: {custom_attr}")
            else:
                print(f"   âŒ å±æ€§ '{inspect_attr}' ä¸å­˜åœ¨")
                print(f"   ğŸ’¡ å¯ç”¨å±æ€§: {[name for name, _ in all_attrs[:10]]}...")
    
    else:
        print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•åˆ†ææ ·æœ¬å±æ€§")

def test_dataset(dataset_name, inspect_attr=None, detailed_tensor='x'):
    """æµ‹è¯•æ•°æ®é›†åŠ è½½"""
    print(f"\n{'='*80}")
    print(f"ğŸ§ª æµ‹è¯•æ•°æ®é›†: {dataset_name}")
    print(f"{'='*80}")
    
    try:
        # è°ƒç”¨æ•°æ®é›†å‡½æ•°
        dataset,__ = performat_SramDataset(
            dataset_dir="/data/tianjn/baseline/basline/basline/",
            name=dataset_name,
            neg_edge_ratio=1.0,
            to_undirected=True,
            task_level='node'
        )
        
        print(f"\nğŸ“Š æ•°æ®é›†åŠ è½½ç»“æœ:")
        print(f"   - åŒ…å«çš„æ•°æ®é›†: {dataset.names}")
        print(f"   - æ€»æ ·æœ¬æ•°: {len(dataset)}")
        print(f"   - ä»»åŠ¡çº§åˆ«: {dataset.task_level}")
        
        # å„å­æ•°æ®é›†ä¿¡æ¯
        if len(dataset.names) > 1:
            print(f"\nğŸ“ˆ å„å­æ•°æ®é›†è¯¦æƒ…:")
            for name in dataset.names:
                if name in dataset.data_lengths:
                    print(f"   - {name}:")
                    print(f"     * æ ·æœ¬æ•°: {dataset.data_lengths[name]}")
                    print(f"     * èµ·å§‹åç§»: {dataset.data_offsets[name]}")
        
        # æ ·æœ¬ä¿¡æ¯
        if len(dataset) > 0:
            print(f"\nğŸ”¬ æ ·æœ¬åˆ†æ:")
            sample = dataset[0]
            print(f"   - èŠ‚ç‚¹æ•°: {sample.num_nodes}")
            print(f"   - è¾¹æ•°: {sample.num_edges}")
            
            if hasattr(sample, 'x') and sample.x is not None:
                print(f"   - èŠ‚ç‚¹ç‰¹å¾ (x): {sample.x.shape}")
                print(f"   - ç‰¹å¾èŒƒå›´: [{sample.x.min():.4f}, {sample.x.max():.4f}]")
            
            # æ£€æŸ¥èŠ‚ç‚¹æ ‡ç­¾/å±æ€§ (y)
            if hasattr(sample, 'y') and sample.y is not None:
                print(f"\nğŸ“Š èŠ‚ç‚¹æ•°æ® (y) è¯¦æƒ…:")
                print(f"   - å½¢çŠ¶: {sample.y.shape}")
                print(f"   - æ•°æ®ç±»å‹: {sample.y.dtype}")
                print(f"   - å€¼èŒƒå›´: [{sample.y.min():.4f}, {sample.y.max():.4f}]")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤šç»´ç‰¹å¾
                if len(sample.y.shape) > 1 and sample.y.shape[1] > 1:
                    print(f"\nğŸ“‹ å¤šç»´ç‰¹å¾åˆ†æ (å…±{sample.y.shape[1]}ç»´):")
                    feature_names = get_feature_names(sample)
                    
                    if feature_names is not None:
                        for i in range(min(sample.y.shape[1], len(feature_names))):
                            print(f"   ç»´åº¦ {i}: {feature_names[i]}")
                    else:
                        print(f"   ç‰¹å¾åç§°: æ— ")
                        print(f"   å„ç»´åº¦:")
                        for i in range(min(10, sample.y.shape[1])):
                            dim_data = sample.y[:, i]
                            print(f"   ç»´åº¦ {i}: æœªçŸ¥ç‰¹å¾ (èŒƒå›´: [{dim_data.min():.4f}, {dim_data.max():.4f}])")
                else:
                    print(f"   - è¿™æ˜¯å•ç»´æ ‡ç­¾æ•°æ®ï¼Œä¸æ˜¯å¤šç»´ç‰¹å¾")
        
        # è°ƒç”¨è¯¦ç»†å±æ€§åˆ†æ
        analyze_dataset_attributes(dataset, inspect_attr, detailed_tensor)
        
        print(f"âœ… æ•°æ®é›† '{dataset_name}' æµ‹è¯•æˆåŠŸ!")
        return dataset
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›† '{dataset_name}' æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_dataloader(dataset):
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨å…¼å®¹æ€§"""
    print(f"\n{'='*60}")
    print("ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½å™¨å…¼å®¹æ€§")
    print(f"{'='*60}")
    
    try:
        from torch_geometric.loader import DataLoader
        
        loader = DataLoader(dataset, batch_size=2, shuffle=True)
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ (æ‰¹æ¬¡å¤§å°: 2)")
        
        # æµ‹è¯•è·å–ä¸€ä¸ªæ‰¹æ¬¡
        for i, batch in enumerate(loader):
            print(f"ğŸ“¦ æ‰¹æ¬¡ {i+1}:")
            print(f"   - æ‰¹æ¬¡èŠ‚ç‚¹æ•°: {batch.num_nodes}")
            print(f"   - æ‰¹æ¬¡è¾¹æ•°: {batch.num_edges}")
            if hasattr(batch, 'batch'):
                print(f"   - æ‰¹æ¬¡å›¾æ•°: {batch.batch.max().item() + 1}")
            
            if i >= 1:  # åªæµ‹è¯•å‰2ä¸ªæ‰¹æ¬¡
                break
        
        print("âœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("="*80)
    print("ğŸš€ performat_SramDataset å‡½æ•°æµ‹è¯•")
    print("="*80)
    
    # æ£€æŸ¥æ–‡ä»¶
    if not prepare_files():
        print("âŒ æ–‡ä»¶æ£€æŸ¥å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return
    
    # ğŸ”§ æ§åˆ¶å“ªä¸ªå¼ é‡å±æ€§æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    # å¯é€‰: 'x', 'y', 'edge_attr', None
    # åªæœ‰æŒ‡å®šçš„å±æ€§ä¼šæ˜¾ç¤ºè¯¦ç»†çš„å­å±æ€§å’Œç‰¹å¾ä¿¡æ¯ï¼Œå…¶ä»–å¼ é‡åªæ˜¾ç¤ºåŸºæœ¬ç»´åº¦ä¿¡æ¯
    detailed_tensor = 'x'  # ä¿®æ”¹è¿™é‡Œæ¥æ§åˆ¶æ˜¾ç¤ºå“ªä¸ªå¼ é‡çš„è¯¦ç»†ä¿¡æ¯
    
    # ğŸ”§ è‡ªå®šä¹‰æ¥å£ï¼šåœ¨è¿™é‡Œä¿®æ”¹è¦æ£€æŸ¥çš„å±æ€§åç§°
    # ä¾‹å¦‚ï¼šinspect_attr = "edge_attr"  # æŸ¥çœ‹è¾¹å±æ€§
    # ä¾‹å¦‚ï¼šinspect_attr = "node_attr"  # æŸ¥çœ‹èŠ‚ç‚¹å±æ€§  
    # ä¾‹å¦‚ï¼šinspect_attr = "edge_label_index"  # æŸ¥çœ‹è¾¹æ ‡ç­¾ç´¢å¼•
    inspect_attr = None  # è®¾ç½®ä¸º None åˆ™ä¸æ£€æŸ¥ç‰¹å®šå±æ€§ï¼Œè®¾ç½®ä¸ºå±æ€§ååˆ™è¯¦ç»†æ£€æŸ¥è¯¥å±æ€§
    
    # æµ‹è¯•ç”¨ä¾‹ - åªæµ‹è¯•åˆå¹¶çš„æ•°æ®é›†
    test_cases = [
        "integrated_position_prediction_graph+integrated_power_density_prediction_graph"  # åªæµ‹è¯•ç»„åˆæ•°æ®é›†
    ]
    
    successful_datasets = []
    
    # é€ä¸ªæµ‹è¯•
    for dataset_name in test_cases:
        dataset = test_dataset(dataset_name, inspect_attr, detailed_tensor)  # ä¼ å…¥è‡ªå®šä¹‰æ£€æŸ¥å±æ€§å’Œè¯¦ç»†å¼ é‡æ§åˆ¶
        if dataset is not None:
            successful_datasets.append((dataset_name, dataset))
    
    # å¦‚æœæœ‰æˆåŠŸçš„æ•°æ®é›†ï¼Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
    if successful_datasets:
        print(f"\n{'='*60}")
        print("ğŸ§ª é¢å¤–æµ‹è¯•ï¼šæ•°æ®åŠ è½½å™¨")
        print(f"{'='*60}")
        
        # ç”¨æœ€åä¸€ä¸ªæˆåŠŸçš„æ•°æ®é›†æµ‹è¯•æ•°æ®åŠ è½½å™¨
        last_dataset_name, last_dataset = successful_datasets[-1]
        print(f"ä½¿ç”¨æ•°æ®é›† '{last_dataset_name}' æµ‹è¯•æ•°æ®åŠ è½½å™¨...")
        test_dataloader(last_dataset)
    
    # æ€»ç»“
    print(f"\n{'='*80}")
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
    print(f"{'='*80}")
    
    print(f"æ€»æµ‹è¯•ç”¨ä¾‹: {len(test_cases)}")
    print(f"æˆåŠŸæ¡ˆä¾‹: {len(successful_datasets)}")
    print(f"å¤±è´¥æ¡ˆä¾‹: {len(test_cases) - len(successful_datasets)}")
    
    print(f"\nâœ… æˆåŠŸçš„æµ‹è¯•:")
    for dataset_name, _ in successful_datasets:
        print(f"   - {dataset_name}")
    
    if len(successful_datasets) == len(test_cases):
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! performat_SramDataset å‡½æ•°å·¥ä½œå®Œç¾!")
        print(f"ğŸ’¡ ä½ å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨è¿™ä¸ªå‡½æ•°ï¼Œæ”¯æŒå•ä¸ªå’Œå¤šä¸ªæ•°æ®é›†ç»„åˆ")
        print(f"\nğŸ”§ è‡ªå®šä¹‰æ£€æŸ¥æ¥å£ä½¿ç”¨æ–¹æ³•:")
        print(f"   åœ¨ main() å‡½æ•°ä¸­ä¿®æ”¹ inspect_attr å˜é‡æ¥æ£€æŸ¥ç‰¹å®šå±æ€§")
        print(f"   ä¾‹å¦‚: inspect_attr = 'edge_attr' æ¥è¯¦ç»†æŸ¥çœ‹è¾¹å±æ€§")
    else:
        print(f"\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()